{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sms', 'label'],\n",
      "        num_rows: 3901\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sms', 'label'],\n",
      "        num_rows: 1004\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['sms', 'label'],\n",
      "        num_rows: 669\n",
      "    })\n",
      "})\n",
      "{'sms': 'You will recieve your tone within the next 24hrs. For Terms and conditions please see Channel U Teletext Pg 750\\n', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as nn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the dataset    \n",
    "train_test_dataset = (load_dataset(\"sms_spam\", split = 'train')).train_test_split(test_size=0.3)\n",
    "test_valid_dataset = train_test_dataset['test'].train_test_split(test_size=0.6)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_dataset['train'],\n",
    "    'test': test_valid_dataset['test'],\n",
    "    'valid': test_valid_dataset['train']})\n",
    "print(dataset)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3901/3901 [00:00<00:00, 19020.56 examples/s]\n",
      "Map: 100%|██████████| 1004/1004 [00:00<00:00, 18337.19 examples/s]\n",
      "Map: 100%|██████████| 669/669 [00:00<00:00, 5177.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sms', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3901\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sms', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1004\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['sms', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 669\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sms\"], truncation=True, padding=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/kevinzhuo/Spam_Detection/venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1115 [2:29:49<?, ?it/s]\n",
      "  2%|▏         | 26/1220 [07:09<4:00:43, 12.10s/it]"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels = 2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=16,   \n",
    "    per_device_eval_batch_size=16,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    evaluation_strategy=\"epoch\",    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                   \n",
    "    args=training_args,                 \n",
    "    train_dataset=tokenized_dataset['train'],      \n",
    "    eval_dataset=tokenized_dataset['valid'],\n",
    "    tokenizer=tokenizer           \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.evaluate(tokenized_dataset['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
